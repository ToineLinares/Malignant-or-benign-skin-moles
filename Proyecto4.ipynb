{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWxLbSuSLKZk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.utils import get_file\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from imblearn.metrics import sensitivity_score, specificity_score\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "cfAdJK9RSzC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RwRW7KKarrK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"benign\", \"malignant\"]\n",
        "def download_and_extract_dataset():\n",
        "  # dataset from https://github.com/udacity/dermatologist-ai\n",
        "  # 5.3GB\n",
        "  train_url = \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip\"\n",
        "  # 824.5MB\n",
        "  valid_url = \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip\"\n",
        "  for i, download_link in enumerate([train_url]):\n",
        "    temp_file = f\"temp{i}.zip\"\n",
        "    data_dir = get_file(origin=download_link, fname=os.path.join(os.getcwd(), temp_file))\n",
        "    print(\"Extracting\", download_link)\n",
        "    with zipfile.ZipFile(data_dir, \"r\") as z:\n",
        "      z.extractall(\"data\")\n",
        "    # remove the temp file\n",
        "    os.remove(temp_file)\n",
        "\n",
        "# comment the below line if you already downloaded the dataset\n",
        "download_and_extract_dataset()\n",
        "\n",
        "def generate_csv(folder, label2int):\n",
        "    folder_name = os.path.basename(folder)\n",
        "    labels = list(label2int)\n",
        "    # generate CSV file\n",
        "    df = pd.DataFrame(columns=[\"filepath\", \"label\"])\n",
        "    i = 0\n",
        "    for label in labels:\n",
        "        print(\"Reading\", os.path.join(folder, label, \"*\"))\n",
        "        for filepath in glob.glob(os.path.join(folder, label, \"*\")):\n",
        "            df.loc[i] = [filepath, label2int[label]]\n",
        "            i += 1\n",
        "    output_file = f\"{folder_name}.csv\"\n",
        "    print(\"Saving\", output_file)\n",
        "    df.to_csv(output_file)\n",
        "\n",
        "# generate CSV files for all data portions, labeling nevus and seborrheic keratosis\n",
        "# as 0 (benign), and melanoma as 1 (malignant)\n",
        "# you should replace \"data\" path to your extracted dataset path\n",
        "# don't replace if you used download_and_extract_dataset() function\n",
        "generate_csv(\"data/train\", {\"nevus\": 0, \"seborrheic_keratosis\": 0, \"melanoma\": 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FevXlNmQMmEE",
        "outputId": "9c7289da-6816-4088-8b8d-6a19de862c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip\n",
            "5736562688/5736557430 [==============================] - 132s 0us/step\n",
            "5736570880/5736557430 [==============================] - 132s 0us/step\n",
            "Extracting https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip\n",
            "Reading data/train/nevus/*\n",
            "Reading data/train/seborrheic_keratosis/*\n",
            "Reading data/train/melanoma/*\n",
            "Saving train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "df_train=pd.read_csv(\"train.csv\")\n",
        "X_train=[]\n",
        "for i in df_train[df_train.keys()[1]]:\n",
        "  img = Image.open(i)\n",
        "  imgGray = (img.convert('L')).resize((10,10))\n",
        " \n",
        "  X_train.append(np.array(imgGray))\n",
        "\n"
      ],
      "metadata": {
        "id": "oBv-TnusN1cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "id": "CfZfvhhMJqvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42510f9d-4cfa-414b-c1b0-dfd1f65762a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train=np.array(X_train)\n",
        "Y_train=np.array(df_train[df_train.keys()[2]])\n",
        "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X_train,Y_train)"
      ],
      "metadata": {
        "id": "dVQ-gc2GKuU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "JOWIKNem2fwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Tiene formato de código\n",
        "```\n",
        "\n",
        "# Nueva sección"
      ],
      "metadata": {
        "id": "AVHtEA1GPugq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal"
      ],
      "metadata": {
        "id": "O48c-KjKPukA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AilHt4swUHyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- Definimos la CNN\n",
        "import math\n",
        "def caract(w,k,p,s):\n",
        "  return math.floor((w-k-2*p)/s +1)\n",
        "w1=10\n",
        "k1=2\n",
        "p1=1\n",
        "s1=1\n",
        "model = torch.nn.Sequential(\n",
        "\n",
        "  torch.nn.Conv2d(1, w1**2, kernel_size=k1, stride=s1, padding=p1),\n",
        "  \n",
        "  torch.nn.ReLU(),\n",
        "\n",
        "  torch.nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "  torch.nn.Dropout(p=0.2),\n",
        "\n",
        "  torch.nn.Conv2d(caract(w1,k1,p1,s1)**2, 2*caract(w1,k1,p1,s1)**2, kernel_size=3, stride=1, padding=2),\n",
        "  # ( (5-3+2*2)/1 ) + 1 = 7   -> 7*7*32\n",
        "\n",
        "  torch.nn.ReLU(),\n",
        "\n",
        "  torch.nn.MaxPool2d(kernel_size=2),\n",
        "  # 7/2 = 3                 -> 3*3*32\n",
        "\n",
        "  torch.nn.Dropout(p=0.2), \n",
        "\n",
        "  torch.nn.Flatten(),\n",
        "  torch.nn.Linear(3*3*32, 10)\n",
        ")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IyDBpePOCv-",
        "outputId": "a9850a3f-2582-4ac1-fb90-470d71c27567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 100, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Dropout(p=0.2, inplace=False)\n",
              "  (4): Conv2d(49, 98, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (5): ReLU()\n",
              "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (7): Dropout(p=0.2, inplace=False)\n",
              "  (8): Flatten(start_dim=1, end_dim=-1)\n",
              "  (9): Linear(in_features=288, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.1)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a4RdHzCVI2h",
        "outputId": "ef73cba3-91b3-4055-d148-4f329fa902db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_ds = torch.utils.data.TensorDataset(torch.from_numpy(X_train1).float(), torch.from_numpy(Y_train1))\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "aHSxzMn9rj7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fv24ie4gThuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--- Visualizamos la estructura de nuestra CNN\n"
      ],
      "metadata": {
        "id": "-mwFhY2edhwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Wbzwz2jJd3m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hiddenlayer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXE18wdAd33e",
        "outputId": "aa3b8bb2-668a-45a5-aece-b487aff91290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hiddenlayer\n",
            "  Downloading hiddenlayer-0.3-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: hiddenlayer\n",
            "Successfully installed hiddenlayer-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento red neuronal\n",
        "\n",
        "import hiddenlayer as hl\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "n_epoch = 10\n",
        "\n",
        "history = hl.History()\n",
        "canvas = hl.Canvas()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "iter = 0\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  for batch_id, (X_train_batch, Y_train_batch) in enumerate(train_dl):\n",
        "\n",
        "    #-- Cargamos los datos en la GPU\n",
        "    X_train_batch, Y_train_batch = X_train_batch.to(device), Y_train_batch.to(device)\n",
        "\n",
        "    model.train()\n",
        "    Xtr = X_train_batch.unsqueeze(1)\n",
        "    Y_pred = model(Xtr[0])\n",
        "\n",
        "    loss = criterion(Y_pred,Y_train_batch)\n",
        "\n",
        "    Y_pred = torch.argmax(Y_pred, 1)\n",
        "\n",
        "    #-- Calculamos el f1 en la cpu\n",
        "    f1 = f1_score(Y_train_batch.cpu(),Y_pred.cpu(), average='macro')\n",
        "\n",
        "    acc = sum(Y_train_batch == Y_pred)/len(Y_pred)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    iter += 1\n",
        "\n",
        "    if iter%10 == 0:\n",
        "        history.log((epoch+1, iter), loss=loss, accuracy=acc)\n",
        "        with canvas:\n",
        "          canvas.draw_plot(history[\"loss\"])\n",
        "          canvas.draw_plot(history[\"accuracy\"])"
      ],
      "metadata": {
        "id": "6jI77qNwUJlX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "560fe6d7-2277-4cfc-a3bd-e1253089990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f81d2f6d173c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mXtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [98, 49, 3, 3], expected input[1, 100, 5, 5] to have 49 channels, but got 100 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-- Validamos el modelo\n",
        "\n",
        "X_test_tensor, Y_test_tensor = torch.from_numpy(X_test).float().to(device), torch.from_numpy(Y_test).to(device)\n",
        "model.eval()\n",
        "Y_pred = model(X_test_tensor.unsqueeze(1))\n",
        "\n",
        "loss = criterion(Y_pred,Y_test_tensor)\n",
        "Y_pred = torch.argmax(Y_pred, 1)\n",
        "\n",
        "f1 = f1_score(Y_test_tensor.cpu(), Y_pred.cpu(), average='macro')\n",
        "\n",
        "acc = sum(Y_test_tensor == Y_pred)/len(Y_pred)\n",
        "\n",
        "print( 'Loss:{:.2f}, F1:{:.2f}, Acc:{:.2f}'.format(loss.item(), f1, acc ) )"
      ],
      "metadata": {
        "id": "QHOHvWTgU31I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "Xs=digits[\"images\"]\n",
        "Ys = digits['target']\n",
        "X_trains, X_tests, Y_trains, Y_tests = train_test_split(Xs,Ys)\n",
        "print(X_train1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B97AU5XnRVM9",
        "outputId": "a48972e0-a54b-46b4-c9eb-ac9c8ae37c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 93 123 117 ... 123 125 122]\n",
            "  [118 127 115 ... 120 126 126]\n",
            "  [124 123 110 ... 115  95 111]\n",
            "  ...\n",
            "  [124 105 117 ... 118 115 115]\n",
            "  [116 127 119 ... 116 122 128]\n",
            "  [126 125 117 ... 117 123 125]]\n",
            "\n",
            " [[179 188 188 ... 166 166 171]\n",
            "  [187 196 190 ... 164 171 177]\n",
            "  [191 196 187 ... 161 173 177]\n",
            "  ...\n",
            "  [186 184 171 ... 159 175 171]\n",
            "  [182 180 171 ... 164 167 167]\n",
            "  [175 176 166 ... 159 159 160]]\n",
            "\n",
            " [[139 146 154 ... 135 151 147]\n",
            "  [151 157 150 ... 137 146 150]\n",
            "  [142 156 131 ... 127 145 154]\n",
            "  ...\n",
            "  [139 140 138 ... 146 161 161]\n",
            "  [145 146 146 ... 159 159 162]\n",
            "  [142 140 147 ... 158 160 163]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[213 208 202 ... 113 178 208]\n",
            "  [209 155 110 ...  93 159 206]\n",
            "  [193 135  82 ... 100 125 188]\n",
            "  ...\n",
            "  [131  70  68 ...  72  98 196]\n",
            "  [161 106  66 ...  94 135 202]\n",
            "  [181 167 149 ... 161 211 216]]\n",
            "\n",
            " [[189 187 185 ... 192 194 196]\n",
            "  [189 188 179 ... 189 192 192]\n",
            "  [191 183 163 ... 186 189 189]\n",
            "  ...\n",
            "  [187 187 137 ... 125 175 182]\n",
            "  [188 189 181 ... 160 179 183]\n",
            "  [187 184 187 ... 180 179 182]]\n",
            "\n",
            " [[141 191 148 ... 170 172  91]\n",
            "  [173 151 110 ... 135 161 138]\n",
            "  [179 129 119 ... 132 139 156]\n",
            "  ...\n",
            "  [149 142 178 ... 175 156 123]\n",
            "  [156 156 186 ... 188 184 106]\n",
            "  [117 197 195 ... 194 177  61]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "jdnjAyqNPrSE"
      }
    }
  ]
}